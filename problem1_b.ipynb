{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to open the video and store the frames in a list named F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the video is stored in the cap variable\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('tag.mp4')\n",
    "if (cap.isOpened() == False):\n",
    "\tprint(\"Unable to open the video file\")\n",
    "else:\n",
    "  print('the video is stored in the cap variable')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STORING THE FRAMES AS PER THE USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All frames stored, now showing one of the frames\n"
     ]
    }
   ],
   "source": [
    "FRAMES = []\n",
    "while(cap.isOpened()):\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    FRAMES.append(frame)\n",
    "  else:\n",
    "    break\n",
    "\n",
    "print('All frames stored, now showing one of the frames')\n",
    "cv2.imshow('i', FRAMES[0])\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function to create homography matrix by using the world detect_corners and the image corners as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_2DHomography_Matrix(W_frame_corners, Cam_frame_corners):\n",
    "    world_c = W_frame_corners\n",
    "    cam_frame_c = Cam_frame_corners\n",
    "    A_mat = np.zeros((8,9))\n",
    "    A_mat = np.matrix(A_mat)\n",
    "    for w,ima in zip(enumerate(world_c), enumerate(cam_frame_c)):\n",
    "    #here x has the coordinates of world and y has the coordinates of image/camera frame\n",
    "        i,j,x,y = w[0], ima[0], w[1], ima[1]\n",
    "        A_mat[i+j,:] = -x[0],-x[1],-1,0,0,0,y[0]*x[0],y[0]*x[1],y[0]\n",
    "        A_mat[(i+j)+1,:] = 0,0,0,-x[0],-x[1],-1,y[1]*x[0],y[1]*x[1],y[1]\n",
    "    U, S, V_T = np.linalg.svd(A_mat)\n",
    "    V_T = np.matrix(V_T)\n",
    "    V = V_T.T\n",
    "    H_gr = V[:,-1]\n",
    "    H_gr = np.reshape(H_gr, [3,3])\n",
    "    H_final = H_gr/H_gr[-1,-1]\n",
    "    return H_final\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse_warping function to project any kind of image using the calculated homography, I have utlized inverse warping to avoid gaps in the image. So, the homography is between the world frame to the camera frame but inverse homography will map it from the camera frame to the world frame. Which agin gave some spots in the image but mean and median blur helped to smoothen out the projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inverse_warping(im, H, size, tes = None):\n",
    "    #creating all the possible indexes, just ;ike the grid method\n",
    "    Y_t, X_t = np.indices((size[0], size[1]))\n",
    "    #flattening of points to create all the possible cooridnates of world frame\n",
    "    world_frame_pts = np.stack((X_t.ravel(), Y_t.ravel(), np.ones(X_t.size)))\n",
    "\n",
    "    H_inv = np.linalg.inv(H)\n",
    "    #conversion of chosen/given world frame to image/camera frame\n",
    "    image_frame_loc = H_inv.dot(world_frame_pts)\n",
    "    image_frame_loc /= image_frame_loc[2,:]\n",
    "\n",
    "    X_image_f, Y_image_f = image_frame_loc[:2,:].astype(int)\n",
    "    # padding\n",
    "    X_image_f[X_image_f >=  im.shape[1]] = im.shape[1]\n",
    "    X_image_f[X_image_f < 0] = 0\n",
    "    Y_image_f[Y_image_f >=  im.shape[0]] = im.shape[0]\n",
    "    Y_image_f[Y_image_f < 0] = 0\n",
    "    \n",
    "    if (type(tes)==np.ndarray):\n",
    "        im[Y_image_f, X_image_f, :] = tes[Y_t.ravel(), X_t.ravel(), :]\n",
    "        return im\n",
    "    else:\n",
    "        image_warp = np.zeros((size[0],size[1], 3))\n",
    "        image_warp[Y_t.ravel(), X_t.ravel(), :]= im[Y_image_f, X_image_f, :]\n",
    "        return image_warp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An algorithm (referenced from online), which help us in detecting all rectangles in the image with some calibration. It was easy to find rectangles and then going ahead with trying to get the homography and decoding. With more tuning, the best results were taken out using orientatation. The corners are given to this function after detection using Shi-Tomashi method.\n",
    "The algorithm is referenced in the report which comes from the idea of APRIL lab in U Mich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_rect(detected_corners:list()):\n",
    "    #an empty matrix to store the pair of points\n",
    "    pair_o_points = []\n",
    "    #now creating combination of all points using inbuilt library (8C2)\n",
    "    for p in itertools.combinations(detected_corners,2):\n",
    "        a,b= p[0],p[1]\n",
    "        distance = np.linalg.norm(a-b)\n",
    "        if(distance>250):\n",
    "            sl = (b[1]-a[1])/(b[0]-a[0])\n",
    "            C =  np.array([(b[0]+a[0])/2,(b[1]+a[1])/2])\n",
    "            pair_o_points.append([a,b,sl,C,distance])\n",
    "    #now after selecting the specific points we will check whether it's a rectangle using diagonal's centre          \n",
    "    sorted_corners = np.array(pair_o_points,dtype=object)\n",
    "    sorted_corners = sorted_corners[np.argsort(sorted_corners[:,-1])][::-1]\n",
    "    selected_rects = []\n",
    "    rect_center = []\n",
    "    #using selected points, now we will make pair of lines and check them\n",
    "    for lines in itertools.combinations(sorted_corners,2):\n",
    "        l1,l2 = lines[0],lines[1]\n",
    "        #checking the difference in length\n",
    "        if(np.abs(l1[-1]-l2[-1])>5):\n",
    "            continue\n",
    "        #checking the intersection of centers\n",
    "        if (np.linalg.norm(l2[3]-l1[3])>5):\n",
    "            continue\n",
    "        rect_center.append(np.array([l1[3],np.linalg.norm(l1[0]-l2[0])]))\n",
    "        selected_rects.append(np.array([tuple(l1[0].tolist()),tuple(l2[0].tolist()),tuple(l1[1].tolist()),tuple(l2[1].tolist())]))\n",
    "        \n",
    "    return selected_rects,rect_center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next part required was to write an algorithm which can try to decode the tags. It can give undesired orientations and ids, which I filtered out in further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_ID_detector(ima):\n",
    "    ima = cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "    #Consider an image with only two distinct image values (bimodal image), \n",
    "    # where the histogram would only consist of two peaks. A good threshold \n",
    "    # would be in the middle of those two values. Similarly, Otsu's method \n",
    "    # determines an optimal global threshold value from the image histogram.\n",
    "    \n",
    "    _,ima = cv2.threshold(ima,127,255,cv2.THRESH_OTSU+cv2.THRESH_OTSU)\n",
    "    #splitting the frame size of 160 into 8 segments\n",
    "    GRID = np.array_split(ima,8)\n",
    "    # print(GRID)\n",
    "    GRID = np.array(GRID)\n",
    "    actual_tag = np.zeros((8,8))\n",
    "    for i,gr in enumerate(GRID):\n",
    "        for j,intern_grid in  enumerate(np.array_split(gr,8,axis=1)):\n",
    "            if np.count_nonzero(intern_grid) < (1/2)*intern_grid.size:\n",
    "                actual_tag[i][j] = 0\n",
    "            else:\n",
    "                actual_tag[i][j] =1\n",
    "    \n",
    "    if actual_tag[5][5]:\n",
    "        orientation = 0\n",
    "    elif actual_tag[2][2]:\n",
    "        orientation = 180\n",
    "    elif actual_tag[2][5]:\n",
    "        orientation = 90\n",
    "    elif actual_tag[5][2]:\n",
    "        orientation = 270\n",
    "    \n",
    "    else :\n",
    "        orientation = None\n",
    "    T_ID = (int(actual_tag[3,3])*1+int(actual_tag[3,4])*2+int(actual_tag[4,4])*4+int(actual_tag[4,3])*8)\n",
    "    T_ID = (((T_ID<<(orientation//90))&0b1111)|(T_ID>>(4-(orientation//90))))\n",
    "    if((actual_tag[0:2]!=0).any() and (actual_tag[:][0:2]!=0).any() and (actual_tag[6:-1][:]!=0).any() and (actual_tag[:][6:-1]!=0).any()):\n",
    "        return np.nan,None,None\n",
    "    \n",
    "\n",
    "    return actual_tag,orientation,T_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the homography and breaking it into intrinsic and extrinsic parameters and then finding the thrid rotation vector, gives us the possibility to extraction 3D rotational and translation matrix, using which we can find the projection matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformation_matrix(K, H):\n",
    "    K_inv = np.linalg.inv(K)\n",
    "    H1, H2 = H[:,0], H[:,1]\n",
    "    lam = (np.linalg.norm(K_inv.dot(H1)) + np.linalg.norm(K_inv.dot(H2)) )/2\n",
    "    lam = 1/lam\n",
    "    B_tilde = lam*np.dot(K_inv, H)\n",
    "    if np.linalg.det(B_tilde) > 0 :\n",
    "        B = B_tilde\n",
    "    else:\n",
    "        B = - B_tilde\n",
    "    R1 = np.array(B[:,0])\n",
    "    R2 = np.array(B[:,1])\n",
    "    r3 = np.cross(R1.T, R2.T)\n",
    "    Trans = B[:,2]\n",
    "    # print(t.shape)\n",
    "    Rot = np.hstack([R1,R2,r3.T])\n",
    "\n",
    "    #P = np.dot(K, M)\n",
    "    return Rot, Trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By multiplying each of the decided vertex for the cube, the cube can be easily projected onto the AR tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_cube(proj_corner, img): \n",
    "    #defining the thickness of cube\n",
    "    thickness = 5\n",
    "    #making a copy of an image \n",
    "    proj_cube_im = np.copy(img)\n",
    "    #segmenting the cube vetex into higher and lower plain\n",
    "    l_p, h_p = np.array_split(proj_corner,2)\n",
    "    h_p = np.resize(h_p,(4,2))\n",
    "    l_p = np.resize(l_p,(4,2))\n",
    "    #drawing the upper and lower line of planes\n",
    "    proj_cube_im = cv2.polylines(proj_cube_im,[l_p],True,(255,0,0),thickness)\n",
    "    proj_cube_im = cv2.polylines(proj_cube_im,[h_p],True,(255,0,0),thickness)\n",
    "    #for joining the vertex of the cubes\n",
    "    for n in zip(l_p,h_p):\n",
    "        lower,upper = n\n",
    "        proj_cube_im = cv2.polylines(proj_cube_im,[np.array([lower,upper])],True,(255,0,0),thickness)\n",
    "    return proj_cube_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corner_detection(ima):\n",
    "    ima = cv2.medianBlur(ima, 7) # Blur the image slightly\n",
    "    gray = cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "\n",
    "    #Perform corner detection\n",
    "    det_corners = cv2.goodFeaturesToTrack(gray,25,0.35,10) \n",
    "    det_corners = np.int0(det_corners)\n",
    "    list_of_corners = []\n",
    "    for i in det_corners:\n",
    "        x,y = i.ravel()\n",
    "        list_of_corners.append(np.array([x,y]))\n",
    "        cv2.circle(img, (x,y), 3, 255, -1)\n",
    "    return list_of_corners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main program starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh/anaconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag id is 7\n"
     ]
    }
   ],
   "source": [
    "Num = 0\n",
    "img_c = np.copy(FRAMES[Num]) #assign the frame of interest\n",
    "img = np.copy(FRAMES[Num])\n",
    "corner = []\n",
    "corner = corner_detection(img)\n",
    "## detect all the rectangle in the image\n",
    "rectangles,d=is_rect(corner)\n",
    "AR_tag = []\n",
    "world_homo_frame = 160 #can be equal to detected edge\n",
    "for i, rect_corn in enumerate(rectangles):\n",
    "    world_frame_corn = [[0,0],[0,world_homo_frame],[world_homo_frame,world_homo_frame],[world_homo_frame,0]]\n",
    "    homo = Get_2DHomography_Matrix(rect_corn,world_frame_corn)\n",
    "    # world_frame_img = cv.warpPerspective(frames[0], homo,(world_homo_frame,world_homo_frame))\n",
    "    world_frame_img = np.uint8(Inverse_warping(img, homo,(world_homo_frame,world_homo_frame)))\n",
    "    homo2 = Get_2DHomography_Matrix(world_frame_corn,world_frame_corn)\n",
    "    #to compensate for the mirror image\n",
    "    world_frame_img = np.uint8(Inverse_warping(np.copy(world_frame_img), homo2,(world_homo_frame,world_homo_frame)))\n",
    "\n",
    "    cv2.imshow(\"world_frame_img\",world_frame_img)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n",
    "    try:\n",
    "        actual_tag,orientation,T_ID = tag_ID_detector(world_frame_img)\n",
    "    except:\n",
    "        continue\n",
    "    AR_tag.append([actual_tag,orientation,T_ID])\n",
    "    print('tag id is', T_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    T_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    testudo = cv2.imread('test.png')\n",
    "    testudo = cv2.resize(testudo, (world_homo_frame, world_homo_frame))\n",
    "    testudo = np.rot90(testudo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    a = Inverse_warping(img, homo, (int(world_homo_frame),int(world_homo_frame)), testudo)\n",
    "    cv2.imshow('im', np.uint8(a))\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of interpolation, the median blur worked really well to smoothen out the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    a = cv2.medianBlur(a, 3)\n",
    "    cv2.imshow('im', np.uint8(a))\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrinsic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    In_param = np.matrix([[1346.100595, 0, 932.1633975], [0, 1355.933136, 654.8986796], [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next part was to get the projection matrix and the vertex of the cube in the image frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    p1,p2 = Transformation_matrix(In_param, np.linalg.inv(homo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    points_to_project = np.float32([[0, 0, 0], \n",
    "                    [160, 0, 0], \n",
    "                    [160, 160, 0], \n",
    "                    [0, 160, 0], \n",
    "                    [0, 0, -160], \n",
    "                    [160, 0, -160], \n",
    "                    [160, 160, -160],\n",
    "                    [0, 160, -160]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    projected_corners, _ = cv2.projectPoints(points_to_project, p1, p2, In_param, np.zeros((1, 4)))\n",
    "    pts = np.int32(projected_corners)\n",
    "    f = np.copy(img_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "    c = project_cube(pts, f)\n",
    "    cv2.imshow('image', np.uint8(c))\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "acd080017cf0dd6132684ffe7508bf5fd61ebd1e63834c4891f264571b6d6a53"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('enpm673_homework1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
